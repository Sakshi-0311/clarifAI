{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMmodel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNg09ZbtsYBbxcjLCSlZw0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spnetic-5/clarifAI/blob/main/LSTMmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi5vZdOMa0il"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Crberyl6N-E"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "import string\r\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTVIf7rE7Jll"
      },
      "source": [
        "response = requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mvM-JnW7Vu_"
      },
      "source": [
        "# response.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q6AkwWLE7sGU",
        "outputId": "77a9461c-d171-4611-9de1-d4b11c98bc86"
      },
      "source": [
        "data = response.text.split('\\n')\r\n",
        "data = data[253:]\r\n",
        "data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  From fairest creatures we desire increase,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeeZzaZog-pc",
        "outputId": "a38f5f27-96b2-4f10-9218-f8c8ad67923b"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anbTLHuJhD2f"
      },
      "source": [
        "data = \" \".join(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNAUG5CwhId8"
      },
      "source": [
        "# Removing Punctuations\r\n",
        "\r\n",
        "def clean_data(doc):\r\n",
        "  tokens = doc.split()\r\n",
        "  table = str.maketrans('', '', string.punctuation)\r\n",
        "  tokens = [ w.translate(table) for w in tokens ]\r\n",
        "  tokens = [word for word in tokens if word.isalpha()]\r\n",
        "  tokens = [word.lower() for word in tokens]\r\n",
        "  return tokens\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXAypU_Thm-x",
        "outputId": "37a74e10-2d24-4ecf-b132-81ac1c966141"
      },
      "source": [
        "tokens = clean_data(data)\r\n",
        "print(tokens[:50])\r\n",
        "# len(tokens) #len(set(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beautys', 'rose', 'might', 'never', 'die', 'but', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', 'his', 'tender', 'heir', 'might', 'bear', 'his', 'memory', 'but', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', 'feedst', 'thy', 'lights', 'flame', 'with', 'selfsubstantial', 'fuel', 'making', 'a', 'famine', 'where', 'abundance', 'lies', 'thy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yZb5Sm3nSGh"
      },
      "source": [
        "length = 50 + 1  #input + output length\r\n",
        "lines = []\r\n",
        "for i in range(length, len(tokens)):\r\n",
        "  seq = tokens[i-length:i]\r\n",
        "  line = ' '.join(seq)\r\n",
        "  lines.append(line)\r\n",
        "  if i > 200000:\r\n",
        "    break\r\n",
        "# print(len(lines)) \r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xj0et1YqzIL"
      },
      "source": [
        "## LSTM Model and prepare x and y\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKRQ2AIBnjyq"
      },
      "source": [
        "import numpy as np\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZgvuIyosEQV"
      },
      "source": [
        "# Tokenization\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(lines) \r\n",
        "sequences = tokenizer.texts_to_sequences(lines) # Assigning every unique word to integer Text->Integer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7AyW6L7sWzX"
      },
      "source": [
        "sequences = np.array(sequences) # Integers -> Arrays\r\n",
        "x, y = sequences[ : ,  :-1], sequences[ : ,-1] # x-> All 50 columns(Not 51) and rows & y-> All rows & Last column(51)\r\n",
        "# x[0], y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBIa-CjPznyy"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1  \r\n",
        "# print(vocab_size)\r\n",
        "y = to_categorical(y,num_classes=vocab_size)\r\n",
        "seq_length = x.shape[1]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqz30e8Q4AFY"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SPpznMQ0Wy5"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))  # Embedding Layer\r\n",
        "model.add(LSTM(100, return_sequences=True)) # 1st LSTM layer with 100 hidden layers\r\n",
        "model.add(LSTM(100)) # 2nd LSTM layer with 100 hidden layers\r\n",
        "model.add(Dense(100, activation='relu'))  #Dense layer with 100 neurons and relu activation\r\n",
        "model.add(Dense(vocab_size, activation='softmax')) #Probability for each predicted word "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZqYcGpa6INe",
        "outputId": "c448f8e4-a7e3-4136-c47d-51ec3206745a"
      },
      "source": [
        "model.summary() #State of model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 50)            650450    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 13009)             1313909   \n",
            "=================================================================\n",
            "Total params: 2,115,259\n",
            "Trainable params: 2,115,259\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7sah-eF6LAb"
      },
      "source": [
        "  # optimizers shape and mold your model into its most accurate possible form by futzing with the weights\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer= 'adam', metrics=['accuracy']) \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tu3ndb38Rd7"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4KAC30L6rnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec96aae-611c-47c6-f0df-3ce973989838"
      },
      "source": [
        "model.fit(x, y, batch_size=256, epochs=100) # 500 epochs for better accuracy\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "782/782 [==============================] - 388s 491ms/step - loss: 7.1975 - accuracy: 0.0283\n",
            "Epoch 2/100\n",
            "782/782 [==============================] - 389s 497ms/step - loss: 6.5386 - accuracy: 0.0408\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 386s 494ms/step - loss: 6.3471 - accuracy: 0.0570\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 387s 494ms/step - loss: 6.1385 - accuracy: 0.0731\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 386s 494ms/step - loss: 5.9720 - accuracy: 0.0840\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 384s 491ms/step - loss: 5.8437 - accuracy: 0.0934\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - 385s 492ms/step - loss: 5.7371 - accuracy: 0.0989\n",
            "Epoch 8/100\n",
            "782/782 [==============================] - 384s 491ms/step - loss: 5.7006 - accuracy: 0.0981\n",
            "Epoch 9/100\n",
            "782/782 [==============================] - 391s 500ms/step - loss: 5.5642 - accuracy: 0.1059\n",
            "Epoch 10/100\n",
            "782/782 [==============================] - 390s 499ms/step - loss: 5.4892 - accuracy: 0.1103\n",
            "Epoch 11/100\n",
            "782/782 [==============================] - 389s 497ms/step - loss: 5.4048 - accuracy: 0.1128\n",
            "Epoch 12/100\n",
            "782/782 [==============================] - 386s 493ms/step - loss: 5.3313 - accuracy: 0.1150\n",
            "Epoch 13/100\n",
            "782/782 [==============================] - 390s 498ms/step - loss: 5.2561 - accuracy: 0.1175\n",
            "Epoch 14/100\n",
            "782/782 [==============================] - 388s 497ms/step - loss: 5.1755 - accuracy: 0.1200\n",
            "Epoch 15/100\n",
            "782/782 [==============================] - 391s 500ms/step - loss: 5.0993 - accuracy: 0.1221\n",
            "Epoch 16/100\n",
            "782/782 [==============================] - 390s 499ms/step - loss: 5.0184 - accuracy: 0.1261\n",
            "Epoch 17/100\n",
            "782/782 [==============================] - 390s 498ms/step - loss: 4.9439 - accuracy: 0.1290\n",
            "Epoch 18/100\n",
            "782/782 [==============================] - 390s 499ms/step - loss: 4.8690 - accuracy: 0.1337\n",
            "Epoch 19/100\n",
            "782/782 [==============================] - 391s 500ms/step - loss: 4.8004 - accuracy: 0.1383\n",
            "Epoch 20/100\n",
            "782/782 [==============================] - 391s 500ms/step - loss: 4.7409 - accuracy: 0.1429\n",
            "Epoch 21/100\n",
            "782/782 [==============================] - 392s 501ms/step - loss: 4.6879 - accuracy: 0.1489\n",
            "Epoch 22/100\n",
            "589/782 [=====================>........] - ETA: 1:36 - loss: 4.6274 - accuracy: 0.1515"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glNT33GQ8zaq"
      },
      "source": [
        "# Prediction\r\n",
        "seed_text=lines[12343]\r\n",
        "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\r\n",
        "  text = []\r\n",
        "  for _ in range(n_words):\r\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
        "    encoded = p\r\n",
        "    ad_sequences(encoded, maxlen =text_seq_length, truncating='pre') # truncating to fix length\r\n",
        "\r\n",
        "    y_predict = model.predict_classes(encoded) # Prediction of Index\r\n",
        "    predicted_word = ''\r\n",
        "    # Finding word of predicted index \r\n",
        "    for word, index in tokenizer.word.index.items():        \r\n",
        "      if index == y_predict:\r\n",
        "        predicted_word = word\r\n",
        "        break\r\n",
        "    seed_text = seed_text  + ' ' + predicted_word\r\n",
        "    text.append(predicted_word)\r\n",
        "  return ' '.join(text)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM0jIBXcFYey"
      },
      "source": [
        "generate_text_seq(model, tokenizer, seq_length, seed_text, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nri_qKGzF1cv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}